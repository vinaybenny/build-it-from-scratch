{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch Series\n",
    "\n",
    "## 2. Artifical Neural  Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will attempt to formulate a intuitive understanding of how an artificial neural network works, along with building and explaining each component that forms part of the neural network. This article assumes that you are familiar with the basic concepts of machine learning and know a bit of elementary statistics and matrix algebra. \n",
    "\n",
    "We'll only use the base numpy and scipy libraries for creating a neural network from scratch. We use these libraries to abstract out the details of basic operations like matrix multiplications & array handling, along with matplotlib for plotting  images.\n",
    "\n",
    "### First, the Neuron\n",
    "\n",
    "We can start off by defining what a single neuron is. A neuron can be thought of as an object that holds a number which signifies how active or excited the object is, much like a volume level in a stereo.  Higher the value of this number, more active the neuron is, and vice versa. And just like the knob or button that controls the volume on the stereo, the neuron object has an input port which can be used to control how active the neuron becomes. For any input supplied through this input port, a mathematical function converts this input value into the activation level of the neuron. This function is rather unimaginatively called the *activation function* of the neuron. The activation or excitation level can be considered as the output from the neuron.\n",
    "\n",
    "For now, let us assume that activation of a neuron is bounded within the range \\[0, 1\\] for any given input. We can think of the state 0 as the state of the neuron being completely inactive, and 1 as the state of neuron being fully activated. So, for any numeric input supplied to the neuron in the interval $[-\\infty, \\infty]$, we need to design an activation function that takes this input number and squishes it into a number between 0 and 1. \n",
    "\n",
    "How do we go about designing this activation function? For now, the only requirement we have is that for any input, the function squishes the input into a number between 0 and 1. There is a very familiar function that maps an arbitrary input into \\[0, 1\\] interval - the **logit** function which is commonly used in logistic regression.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{-k(x - x_0)} }\n",
    "\\end{align}\n",
    "\n",
    "For keeping things simple, we use k=1, and x<sub>0</sub> = 0 as default for now, which makes the equation\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{-x} } = \\sigma(x)\n",
    "\\end{align}\n",
    "\n",
    "We call this special case of the logit function as the **sigmoid**, and use a $\\sigma$ to represent it. Let us write some code to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.cell:nth-child(10) .output {\n",
       "    flex-direction: row;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mtl\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from IPython.display import display, HTML, Markdown # For printing nice looking tables from pandas dataframes\n",
    "from scipy import optimize # We will use this for mathematical optimisations later on\n",
    "\n",
    "# For printing dataframe tables side by side\n",
    "CSS = \"\"\"\n",
    "div.cell:nth-child(10) .output {\n",
    "    flex-direction: row;\n",
    "}\n",
    "\"\"\"\n",
    "HTML('<style>{}</style>'.format(CSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x220b7022550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-1.* x) )\n",
    "\n",
    "pyplot.plot(np.arange(-10, 10, 0.1), sigmoid(np.arange(-10, 10, 0.1)))\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "A neural network, as the name suggests, is a set of interconnected neurons. In biological neural networks, individual neuron cells are connected to each other through connections called synapses. Each synapse has a weight associated with it; this weight determines how strongly two neurons are connected to each other. A signal passed from one neuron to the next through this synapse is amplified (or attenuated) proportional to the weight of the synaptic connection.\n",
    "\n",
    "just as in the biological counterpart, the artificial neural network we are constructing here can be be thought of as a bunch of artificial neurons that are linked to each other, with each link having an associated strength. When the output from one neuron passes to the subsequent neuron, it gets scaled by the strength of the link that connects the two neurons. This strength is called the **weight** of the link, and the weight is also just a number, just like neuron activation levels.\n",
    "\n",
    "As an example, consider the simplest case with 2 neurons connected to each other through a link, and let the weight of the link be $w$. For an input $x$ to neuron A, the activation function $\\sigma_A$ squishes $x$ into an number between 0 and 1, and this  is then passed to the synaptic link. The number is then scaled by the weight $w$ of the link, and becomes the input to neuron B. Now the activation function $\\sigma_B$ of neuron B picks this value up and squishes it to \\[0, 1\\], which is the output of neuron B.\n",
    "\n",
    "Hence the final output $y$ is given by  $ y = \\sigma_B( \\sigma_A(x) * w) ) $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large input: x=20 gives  0.982013789892\n",
      "Small input: x=-20 gives  0.500000002061\n"
     ]
    }
   ],
   "source": [
    "# Create a 2 neuron network, with 'x' as input and 'w' as weight of the link between the 2 neurons\n",
    "def simplest_neural_network(x, w):\n",
    "    return sigmoid(sigmoid(x)*w)\n",
    "\n",
    "print(\"Large input: x=20 gives \", simplest_neural_network(x = 20, w = 4)) \n",
    "print(\"Small input: x=-20 gives \", simplest_neural_network(x = -20, w = 4)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start conceptualising slightly more complex neural network structures. Before we consider the neural network structure, consider the following problem- let's say we are supplied a set of $n$ numbers, and are asked to design a function that takes these $n$ numbers as input and produces a given output $y$. In mathematical terms, we are asked to guess function $F$ in the following equation-\n",
    "\n",
    "\\begin{align}\n",
    "y = F(x_1, x_2, ..., x_n)\n",
    "\\end{align}\n",
    "\n",
    "To make the function a bit easier to guess, let's say we are given several such input-output pairs that the function should produce. Suppose that we are given $m$ such pairs of inputs and outputs- in that case, we can rewrite the equation in matrix terms -\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "y_{21} \\\\\n",
    ": \\\\\n",
    "y_{m1} \\\\\n",
    "\\end{bmatrix}\n",
    "= F(\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & ... & x_{1n} \\\\\n",
    "x_{21} & x_{22} & ... & x_{2n} \\\\\n",
    ": & : & : & : \\\\\n",
    "x_{m1} & x_{m2} & ... & x_{mn} \\\\\n",
    "\\end{bmatrix} \n",
    ")\n",
    "\\end{align}\n",
    "\n",
    "which can be rewritten as\n",
    "\n",
    "\\begin{align}\n",
    "Y = F(X)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$F$ is a function that acts on each row of the input matrix $X$ and produces each element of the output $Y$ vector. And how on earth would we go about guessing the parameters of this function? If you're familiar with statistics, then you'd know that there are plenty of parameter estimation techniques that you can choose from, like linear least squares estimators or maximum likelihood functions depending on the nature of the problem. And if you're a machine learning enthusiast, you'd probably pick techniques like SVMs or decision trees or neural networks. Both groups of techniques share the same mathematical foundations of optimisation albeit with different assumptions, and each technique offers its own pros and cons. Since we are discussing neural networks in this article, let us try to design a neural network structure that solves the function estimation problem.\n",
    "\n",
    "To start with, we can imagine an easily comprehensible structure for the neural network (as opposed to imagining it as a messy interconnection of neurons like structure of our own brains). We are going to put 3 layers of neurons in our network-\n",
    "\n",
    "1. an input layer of neurons (this is where we feed in our data inputs), \n",
    "2. a middle layer of neurons which are connected to the input layer through a set of weighted links (called the hidden layer), \n",
    "3. an output layer of neurons which are connected to the middle layer through another set of linked weights.\n",
    "\n",
    "Obviously the input layer will have $n$ neurons, so as to accept the $n$ separate columns (or variables) in our input dataset. The choice of number of neurons in the hidden layer is arbitrary, and we will think of ways to choose this at a later stage. The output layer has one neuron for now, since we have only one column of output in the above example. However, note that there is no reason why the output has to be unidimensional; in fact we can have neural networks with multiple outputs, in which case the original problem becomes finding a function that relates input-output vector pairs.\n",
    "\n",
    "Why did we choose a network with three layers? Because obviously with just one layer of neurons there is no network that can be formed (we do not connect neurons within the same layer), and with two layers, what we get is a simplistic network that can only guess functions that are a linear combination of the input data. This fails to capture any non-linear relationships that may exist between the input and output.\n",
    "\n",
    "Here, we link every input neuron with every hidden layer neuron, and the same is true for the linking between the hidden layer and the output layer. This means that if there are $n$ input neurons and $h$ hidden layer neurons, the there are $n*h$ links between the two layers. Similarly, if there are $p$ output neurons, then there are $h*p$ connections between hidden layer and output layer. The outputs from the output layer form the result generated by the network. As stated earlier, we give each synaptic link a weight associated with it, which represents how strongly the two neurons are linked. As an initial guess for how strong these links should be, we can initialise all the weights to random numbers drawn from a standard normal distribution.\n",
    "\n",
    "Next, we code up the neural network structure and initialise the weights of the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />Weight Matrix between input and hidden layer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204708</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>-0.519439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.555730</td>\n",
       "      <td>1.965781</td>\n",
       "      <td>1.393406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -0.204708  0.478943 -0.519439\n",
       "1 -0.555730  1.965781  1.393406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />Weight Matrix between hidden and output layer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.769023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.092908\n",
       "1  0.281746\n",
       "2  0.769023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Neuralnetwork():\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        np.random.seed(12345)\n",
    "        \n",
    "        # For now, we restrict ourselves to 1 hidden layer for simplicity's sake\n",
    "        self.input_layer_size = config[0]\n",
    "        self.output_layer_size = config[2]\n",
    "        self.hidden_layer_size = config[1]\n",
    "        \n",
    "        # Since we have 3 layers, we will have 2 sets of weights\n",
    "        # Initialise all these weights to 1 and biases to 0 - for now.\n",
    "        self.weight_list = [np.random.randn(self.input_layer_size, self.hidden_layer_size),\n",
    "                                 np.random.randn(self.hidden_layer_size, self.output_layer_size)]\n",
    "        return\n",
    "    \n",
    "# Create a test neural network with 2 inputs, 3 neurons in hidden layer and 1 output\n",
    "neuralnet = Neuralnetwork([2,3,1])\n",
    "#print(*neuralnet.weight_list, sep='\\n\\n')\n",
    "\n",
    "# Display the weight matrices\n",
    "display(Markdown(\"<br />Weight Matrix between input and hidden layer:\"),pd.DataFrame(neuralnet.weight_list[0]))\n",
    "display(Markdown(\"<br />Weight Matrix between hidden and output layer:\"), pd.DataFrame(neuralnet.weight_list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "Now that we have defined a complicated but well-organised neural network structure, how do we go about feeding inputs and obtaining outputs from the neural network? We will design a process by which the input layer accepts our inputs, and feeds it into successive layers through the links until we get the outputs from the output layer. It may look mathematically cumbersome to take each input row from the input matrix, multiply with the weights of each link it passes through, then propagating these results forward to next layer of neurons, applying the activation functions, and repeating the process until we reach the output layer; fortunately, matrix algebra comes to our aid.\n",
    "\n",
    "We can implement this forward pass of inputs through the neural network layers as a set of successive matrix multiplications, interspersed with the application of activation functions on the output of each layer. The sequence of operations will be clear from the following table:\n",
    "\n",
    "| Notation | Dimensions | Description |\n",
    "|:---------------------:|:---:|:---:|\n",
    "| $X$ | $m \\times n$ |   Input matrix with $m$ rows and $n$ columns|\n",
    "| $Y$ | $m \\times p$ |   The given output matrix with $m$ rows and $p$ columns|\n",
    "| $W^{(0)}$ | $n \\times h_1$ |   Weights connecting input layer to the hidden layer with $h_1$ neurons|\n",
    "| $Z^{(0)} = X W^{(0)}$ | $m \\times h_1$ |   The signals coming into the hidden layer neurons|\n",
    "| $A^{(0)} = \\sigma(Z^{(0)})$ | $m \\times h_1$ |   The set of activation vectors for hidden layer|\n",
    "| $W^{(1)}$ | $h_1 \\times p$ |   Weights connecting hidden layer to the output layer with $p$ columns|\n",
    "| $Z^{(1)} = A^{(0)} W^{(1)}$ | $m \\times p$ |   The signals coming into the output layer|\n",
    "| $ \\hat{Y} = \\sigma(Z^{(1)})$ | $m \\times p$ |   The set of activation vectors for output layer which form the output|\n",
    "\n",
    "\n",
    "\n",
    "Next we implement this forward propagation in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />**$X$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  0\n",
       "1  0  1\n",
       "2  1  0\n",
       "3  1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**$\\hat{Y}$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.842851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.851076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.787807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.842851\n",
       "1  0.775764\n",
       "2  0.851076\n",
       "3  0.787807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**${Y}$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forwardprop(neuralnet, X):\n",
    "    # For a given input array, calculate the activations of each successive layer of neurons in the network.\n",
    "    # For this, multiply the activations of the first layer with the weights, and add the biases of the next layer; then apply\n",
    "    # the sigmoid function to obtain the activations of the next layer. Repeat this successively until we get to\n",
    "    # the output layer\n",
    "    i=0\n",
    "    activation_vector = X\n",
    "    neuralnet.z_vectors = [ [], [] ]\n",
    "    neuralnet.activation_vectors = [ [], [], [] ]\n",
    "    neuralnet.activation_vectors[i].append(activation_vector)\n",
    "    \n",
    "    for weight in neuralnet.weight_list:\n",
    "        z_vector = np.dot(activation_vector, weight)\n",
    "        activation_vector = sigmoid(z_vector)\n",
    "        neuralnet.z_vectors[i].append(z_vector)\n",
    "        neuralnet.activation_vectors[i+1].append(activation_vector)            \n",
    "        i = i+1\n",
    "\n",
    "    yhat = activation_vector\n",
    "    return yhat\n",
    "\n",
    "# We create a simple logical XOR gate truth table as the input data to the neural network.\n",
    "x = np.array([ [0,0], [0,1], [1,0], [1,1]])  \n",
    "y = np.array([ [0], [1], [1], [0]])\n",
    "\n",
    "# Create a test neural network that accepts our defined data.\n",
    "neuralnet = Neuralnetwork([x.shape[1], 2, y.shape[1]])\n",
    "yhat = forwardprop(neuralnet, x)\n",
    "\n",
    "# Print the activation vectors from each layer\n",
    "#print(*yhat, sep = \"\\n\\n\")\n",
    "\n",
    "# View output from feed forward step\n",
    "display(Markdown(\"<br />**$X$:**\"), pd.DataFrame(x))\n",
    "display(Markdown(\"<br />**$\\hat{Y}$:**\"), pd.DataFrame(yhat))\n",
    "display(Markdown(\"<br />**${Y}$:**\"), pd.DataFrame(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the output vector $\\hat{Y}$ we obtain from the neural network is complete junk and is nowhere close to the actual output $Y$ vector. This is because we chose the weights of the neural network quite randomly. What we need to do is to train the neural network to give us our expected output for a specfic input. In other words, we want the neural network to guess the underlying function that relates each row in the input matrix $X$ to the corresponding output in vector $Y$. To do this, we need to show the neural network several instances of the inputs and expected outputs, based on which the neural network automatically adjusts its weights to give us an output that is as close as possible to the expected output.\n",
    "\n",
    "How do we do this? This is where the backpropagation algorithm comes in.\n",
    "\n",
    "### Back propagation\n",
    "\n",
    "We finally get to the difficult part of the neural network design, which involves training the network to give us the output we want.\n",
    "\n",
    "First, the network needs to know it is doing a terrible job. We can do this by designing a cost metric, which the network uses to measure how badly it is performing. Next, it needs a way to adjust itself to give better estimates based on this cost metric.\n",
    "\n",
    "Let us start by defining the cost function. Here again, statistics comes to our rescue. We pick the most commonly used cost metric in statistics- the sum of squared differences between the obtained output and the expected output. To obtain the cost $C$, we take the square of the difference between each corresponding elements in the expected output $Y$ and the obtained output $\\hat{Y}$. As stated earlier, there is no restriction on $Y$ and $\\hat{Y}$ to be vectors- these are special cases when there is only one output neuron. There can be multiple output neurons in a neural network, and in such cases $Y$ and $\\hat{Y}$ can be considered as matrices with each column representing each output neuron activation. In general terms, we can consider $Y$ and $\\hat{Y}$ as output matrices (in cases where there are more than one output neuron).\n",
    "\n",
    "In mathematical terms, the cost function can be represented as the trace of the square of the error matrix. The diagonal of the matrix consists of the square of each element in $Y-\\hat{Y}$, and the trace gives us its sum.\n",
    "\n",
    "\\begin{align}\n",
    "C & = tr( (Y - \\hat{Y} )^T (Y - \\hat{Y} ) )\n",
    "\\end{align}\n",
    "\n",
    "The cost function represents how far away we are from the expected output. Our objective is to minimise this cost function, and we must look for ways we can do this. On examining the equation, we find that while $Y$ is fixed and given to us, $\\hat{Y}$ is calculated through a series of matrix operations in the forward propagation algorithm using the input data $X$, the intermediate weight vectors between the neuron layers $W^{(i)}$ and the activation function $\\sigma$. Again, $X$ is fixed, whereas we have limited our activation function to be the sigmoid, which leaves us with one choice- modifying the synaptic weights.\n",
    "\n",
    "Our objective hence becomes minimising the cost function with respect to the synaptic weights in the network. Minimization of a function involves knowing the gradient of that function with respect to the independent variables. Since we have a function with several weights, our cost gradient vector becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla C = \\begin{bmatrix}\n",
    "\\frac{\\partial C}{\\partial{W^{(0)}}} \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(1)}}} \\\\\n",
    ".. \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In our case,we have 2 sets of weights- the ones between input and hidden layer, and the one between hidden and otput layers. Using the chain rule of differentiation, we can decompose the cost gradient vector as \n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} = \\frac{\\partial C}{\\partial{A^{(L)}}} \\cdot \\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}} \\cdot \\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}} }\n",
    "\\end{align}\n",
    "<br></br>\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L-1)}}} = \\frac{\\partial C}{\\partial{A^{(L)}}} \\cdot \\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}} \\cdot \\frac{\\partial{Z^{(L)}}}{\\partial{A^{(L-1)}}} \\cdot \\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}} \\cdot \\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}} }\n",
    "\\end{align}\n",
    "\n",
    "Here, we can substitute each of these partial derivatives using the following- \n",
    "\n",
    "| Derivatives                     | Description|\n",
    "|:--------------------------------|:-----------|\n",
    "|$\\frac{\\partial{C}}{\\partial{A^{(L)}}}=-2(Y-\\hat{Y})$| Partial derivative of cost function with respect to $\\hat{Y}$  |\n",
    "|$\\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}}=\\sigma^{'}(Z^{(L)})$| Partial derivative of sigmoid activation function $\\sigma$ with respect to pre-activation vector $Z^{(L)}$  |\n",
    "|$\\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}}}=A^{(L-1)}$|Partial derivative of pre-activation vector $Z^{(L)}$ with respect to weights $W^{(L)}$|\n",
    "|$\\frac{\\partial{Z^{(L)}}}{\\partial{A^{(L-1)}}}=W^{(L)}$|Partial derivative of pre-activation vector $Z^{(L)}$ with respect to previous hidden layer activations $A^{(L-1)}$|\n",
    "|$\\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}}=\\sigma^{'}(Z^{(L-1)})$|Partial derivative of sigmoid function $\\sigma$ with respect to pre-activation vector $Z^{(L-1)}$ for hidden layer neurons |\n",
    "|$\\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}}} = X$|Input Data |\n",
    "\n",
    "Based on these substitutions, we can rewrite the above equations as\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} = -2(Y-\\hat{Y}) \\cdot \\sigma^{'}(Z^{(L)}) \\cdot A^{(L-1)}\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L-1)}}} = -2(Y-\\hat{Y}) \\cdot \\sigma^{'}(Z^{(L)}) \\cdot W^{(L)} \\cdot \\sigma^{'}(Z^{(L-1)}) \\cdot X\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "As a general note, in case we have further hidden layers, this repeated application of chain rule differentiation is continued until we get the entire cost gradient vector. This can be automated easily enough, but for simplicity's sake we keep our implementation limited to the two layers for now. \n",
    "\n",
    "All that remains is to code up the cost function, its gradient function, and the backpropagation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost(y, yhat):\n",
    "    return np.square(y - yhat)\n",
    "\n",
    "def cost_prime(y, yhat):\n",
    "    return -1*(y - yhat)\n",
    "\n",
    "def sigmoid_prime(z_vector):\n",
    "    return np.exp(np.multiply(z_vector, -1))/((1+np.exp(np.multiply(z_vector, -1)))**2)\n",
    "\n",
    "def backprop(neuralnet, output_vector):\n",
    "    \n",
    "    # Equation 1\n",
    "    dCdZ_L = np.multiply(cost_prime(output_vector, neuralnet.z_vectors[-1]), sigmoid_prime(neuralnet.z_vectors[-1]))\n",
    "    dCdW_L = np.dot(np.transpose(neuralnet.activation_vectors[-2][0]), dCdZ_L[0])\n",
    "    \n",
    "    # Equation 2\n",
    "    dCdZ_L1 = np.dot(dCdZ_L[0], np.transpose(neuralnet.weight_list[-1])) * sigmoid_prime(neuralnet.z_vectors[-2])\n",
    "    dCdW_L1 = np.dot(np.transpose(neuralnet.activation_vectors[0][0]), dCdZ_L1[0])\n",
    "\n",
    "    return np.concatenate((dCdW_L1.ravel(), dCdW_L.ravel()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14058941  0.10727097  0.11407183  0.08981533  0.24075876  0.29011585]\n"
     ]
    }
   ],
   "source": [
    "#Test code  \n",
    "x = np.array([ [0,0], [0,1], [1,0], [1,1]])  \n",
    "y = np.array([ [0], [1], [1], [0]])\n",
    "\n",
    "neuralnet = Neuralnetwork([x.shape[1], 2, y.shape[1]])\n",
    "yhat = forwardprop(neuralnet, x)\n",
    "print(backprop(neuralnet, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Gradient Descent\n",
    "\n",
    "The function of the backpropagation algorithm is to propagate the error backwards through the neural network layers, and thereby calculate the gradient of all the weight vectors. Now we know the slope of the cost function with respect to each weight in the network; i.e., for a tiny change in the any particular weight, we get the corresponding change in cost value. Since we are hoping to minimise the cost function, we need to adjust each of the weights in the direction that causes the cost to decrease- and this direction is evident from the sign of the gradient for that particular weight. If gradient is negative, we need to increase that weight, and vice-versa. Larger the negative cost gradient with respect to a weight, more sensitive is the cost to any change in that weight value.\n",
    "\n",
    "Now that we know the direction in which we adjust the weight, what should be the magnitude of change that we need to apply? This is a tricky choice, and one that is governed by heuristics of the problem. For now, we will ignore this complication and assume that we decrease each weight by an arbitrarily chosen value of 7 times the cost gradient. We need to repeatedly apply forward and backpropagation to minimize the cost iteratively, until we tune the weights to get close to the expected output. How many iterations do we need. Here again, We arbitrarily choose the number of iterations of minimzation to 1000 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  1.40349710463\n",
      "Error:  1.02693721685\n",
      "Error:  0.810291352279\n",
      "Error:  0.753403391324\n",
      "Error:  0.73333307472\n",
      "Error:  0.722406823856\n",
      "Error:  0.715290869766\n",
      "Error:  0.710178506928\n",
      "Error:  0.70627087171\n",
      "Error:  0.703153989538\n",
      "Error:  0.700589290581\n",
      "Error:  0.698428371649\n",
      "Error:  0.696573433645\n",
      "Error:  0.694957047828\n",
      "Error:  0.693531004079\n",
      "Error:  0.692259787752\n",
      "Error:  0.69111657409\n",
      "Error:  0.690080667121\n",
      "Error:  0.68913580517\n",
      "Error:  0.688269006412\n"
     ]
    }
   ],
   "source": [
    "def train(neuralnet, X, y, iterations = 1000):\n",
    "    \n",
    "    for loopcounter in range(0, iterations):\n",
    "        yhat = forwardprop(neuralnet, X)\n",
    "        grad = backprop(neuralnet, y)\n",
    "        #print((neuralnet.weight_list[0].ravel(),neuralnet.weight_list[1].ravel()))\n",
    "        #print(grad)\n",
    "        if loopcounter%50 == 0:\n",
    "            print(\"Error: \", np.sum(cost(y, yhat)))\n",
    "        W1_start = 0\n",
    "        W1_end = neuralnet.hidden_layer_size * neuralnet.input_layer_size        \n",
    "        W1 = np.reshape(grad[W1_start:W1_end], (neuralnet.input_layer_size , neuralnet.hidden_layer_size))\n",
    "        \n",
    "        W2_end = W1_end + neuralnet.hidden_layer_size*neuralnet.output_layer_size\n",
    "        W2 = np.reshape(grad[W1_end:W2_end], (neuralnet.hidden_layer_size, neuralnet.output_layer_size))\n",
    "        neuralnet.weight_list = [neuralnet.weight_list[0] - 7*W1, neuralnet.weight_list[1] - 7*W2]\n",
    "        #print((neuralnet.weight_list[0].ravel(),neuralnet.weight_list[1].ravel()))\n",
    "    return\n",
    "\n",
    "#Test code  \n",
    "x = np.array([ [0,0], [0,1], [1,0], [1,1]])  \n",
    "y = np.array([ [0], [1], [1], [0]])\n",
    "neuralnet = Neuralnetwork([x.shape[1], 2, y.shape[1]])\n",
    "train(neuralnet, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.50069571],\n",
      "       [ 0.72565667],\n",
      "       [ 0.72565763],\n",
      "       [ 0.5350336 ]])]\n"
     ]
    }
   ],
   "source": [
    "# Final output\n",
    "print(neuralnet.activation_vectors[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we observe here that using our simple training algorithm, we managed to progressively reduce the error to get closer to the required output. However, with our naive training algorithm, the convergence is extremely slow and our outputs do not get close to our required outputs fast enough. Can we do better? Yes, with better training algorithms. in the next step, we will replace our iterative training with the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm for to achieve faster minimization of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
