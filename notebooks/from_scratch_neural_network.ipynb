{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch Series\n",
    "\n",
    "## 2. Neural  Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will attempt to formulate a intuitive understanding of what exactly a neural network is, along with building and explaining each component that forms part of the neural network.\n",
    "\n",
    "As part of this exercise, we will use the MNIST dataset which is a collection of images of handwritten digits and teach a neural network to recognise handwritten digits by analysing the pixels of the image.\n",
    "\n",
    "First, we load up our numpy libraries for abstracting out basic operations like matrix multiplications & array handling, along with matplotlib for plotting  images. We also use tensorflow here, which is not strictly required - we simply use it because it saves us a lot of effort in having to write code to load the MNIST datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mtl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from IPython.display import display, HTML, Markdown # For printing nice looking tables from pandas dataframes\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use tensorflow functions to automatically download the MNIST data from its hosting website, and load it into memory as a \"Dataset\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what the data looks like first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.12156864,  0.51764709,\n",
       "         0.99607849,  0.99215692,  0.99607849,  0.83529419,  0.32156864,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.08235294,\n",
       "         0.55686277,  0.91372555,  0.98823535,  0.99215692,  0.98823535,\n",
       "         0.99215692,  0.98823535,  0.87450987,  0.07843138,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.48235297,  0.99607849,  0.99215692,  0.99607849,\n",
       "         0.99215692,  0.87843144,  0.7960785 ,  0.7960785 ,  0.87450987,\n",
       "         1.        ,  0.83529419,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.7960785 ,\n",
       "         0.99215692,  0.98823535,  0.99215692,  0.83137262,  0.07843138,\n",
       "         0.        ,  0.        ,  0.2392157 ,  0.99215692,  0.98823535,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.16078432,  0.95294124,  0.87843144,  0.7960785 ,\n",
       "         0.71764708,  0.16078432,  0.59607846,  0.11764707,  0.        ,\n",
       "         0.        ,  1.        ,  0.99215692,  0.40000004,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.15686275,  0.07843138,  0.        ,  0.        ,  0.40000004,\n",
       "         0.99215692,  0.19607845,  0.        ,  0.32156864,  0.99215692,\n",
       "         0.98823535,  0.07843138,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32156864,  0.83921576,  0.12156864,\n",
       "         0.44313729,  0.91372555,  0.99607849,  0.91372555,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.24313727,  0.40000004,  0.32156864,\n",
       "         0.16078432,  0.99215692,  0.90980399,  0.99215692,  0.98823535,\n",
       "         0.91372555,  0.19607845,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.59607846,  0.99215692,  0.99607849,  0.99215692,  0.99607849,\n",
       "         0.99215692,  0.99607849,  0.91372555,  0.48235297,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.59607846,  0.98823535,\n",
       "         0.99215692,  0.98823535,  0.99215692,  0.98823535,  0.75294125,\n",
       "         0.19607845,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.24313727,  0.71764708,  0.7960785 ,  0.95294124,\n",
       "         0.99607849,  0.99215692,  0.24313727,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.15686275,  0.67450982,  0.98823535,\n",
       "         0.7960785 ,  0.07843138,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.08235294,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.71764708,  0.99607849,  0.43921572,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.24313727,\n",
       "         0.7960785 ,  0.63921571,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.2392157 ,  0.99215692,  0.59215689,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.08235294,  0.83921576,  0.75294125,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.04313726,  0.83529419,  0.99607849,\n",
       "         0.59215689,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.40000004,\n",
       "         0.99215692,  0.59215689,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.16078432,\n",
       "         0.83529419,  0.98823535,  0.99215692,  0.43529415,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.16078432,  1.        ,  0.83529419,\n",
       "         0.36078432,  0.20000002,  0.        ,  0.        ,  0.12156864,\n",
       "         0.36078432,  0.67843139,  0.99215692,  0.99607849,  0.99215692,\n",
       "         0.55686277,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.67450982,  0.98823535,  0.99215692,  0.98823535,\n",
       "         0.7960785 ,  0.7960785 ,  0.91372555,  0.98823535,  0.99215692,\n",
       "         0.98823535,  0.99215692,  0.50980395,  0.07843138,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.08235294,\n",
       "         0.7960785 ,  1.        ,  0.99215692,  0.99607849,  0.99215692,\n",
       "         0.99607849,  0.99215692,  0.95686281,  0.7960785 ,  0.32156864,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.07843138,  0.59215689,\n",
       "         0.59215689,  0.99215692,  0.67058825,  0.59215689,  0.59215689,\n",
       "         0.15686275,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[1:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the MNIST dataset\n",
    "\n",
    "The MNIST dataset is a labelled collection of images of digits from 0 to 9, which were handwritten by different individuals and then photographed and painstakingly labelled. Identifying handwritten numbers are usually a very simple task for human beings in most cases, so we tend to underestimate the complexity of how we perform visual recognition, which is a product of millions of years of evolution. Since computers are designed to follow a set of instructions in a strict order, the task of visual recognition needs to be broken down into a set of fixed business rules that the computer needs to check before making an inference. However, given that everyone has their own distinctive style of writing numbers, it is very difficult to formulate a commbination of business rules that accounts for every possibility without adding enormous complexity to the visual recognistion problem. This is where the utility of neural networks come in.\n",
    "\n",
    "The tensorflow helper scripts read in the MNIST images (a set of 55,000 images), each of which is 28 by 28 pixels. The images are stored in terms of pixels, each of which are stored as greyscale values on 28 * 28 = 784 columns. There are 55000 such images, which form 55000 rows.\n",
    "\n",
    "First, we'll write a function that reads in a row of the dataset and create an image from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(pixels, width, height):\n",
    "    \n",
    "    image = pixels.reshape(width, height)\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.imshow(image, cmap = mtl.cm.Greys)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25746a7eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(mnist.train.images[1,:], 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. We know this is a 3, but the computer doesn't, yet. \n",
    "\n",
    "### First, the Neuron\n",
    "\n",
    "We can start off by defining what a single neuron is. A neuron can be thought of as an object that holds a number - and this number signifies how active or excited the object is.  Higher the value of this number, more active the neuron is, and vice versa. The neuron object has an input port which can be used to control how excited the neuron becomes. A function that takes this input and turns it into the activation level of the neuron is called the activation function. This activation or excitation level then becomes the output of the neuron.\n",
    "\n",
    "For now, let us assume that the limits of how excited a neuron can become as a result of any input is bounded within the range \\[0, 1\\]. So, for any numeric input to the neuron, we need to design an activation function that takes this numerical input and squishes it into a number between 0 and 1. We can think of the state 0 as the state of the neuron being completely inactive, and 1 as the state of neuron being fully active. \n",
    "\n",
    "Let us design this activation function now. For now, the only requirement we have is that for any input, the function squishes the input into a number between 0 and 1. There is a very familiar function that maps an arbitrary input into \\[0, 1\\] interval - the **logit** function which is commonly used in logistic regression.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{-k(x - x_0)} }\n",
    "\\end{align}\n",
    "\n",
    "For keeping things simple, we use k=1, and x<sub>0</sub> = 0 as default for now, which makes the equation\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{x} } = \\sigma(x)\n",
    "\\end{align}\n",
    "\n",
    "We call this special case of the logit function as the **sigmoid**, and use a $\\sigma$ to represent it. Let us write some code to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7iIiWUjlLiKShf4/YXaz4eQemOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183da7a37b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-1.* x) )\n",
    "\n",
    "pyplot.plot(np.arange(-10, 10, 0.1), sigmoid(np.arange(-10, 10, 0.1)))\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "A neural network, as the name suggests, is a set of interconnected neurons like the one we just created. As you may already be aware, such a network is inspired from biological nervous systems which consist of interlinked neuron cells, and the strengths of these connections may vary depending on various factors.\n",
    "\n",
    "The artificial neural network we are constructing here also be be thought of as a bunch of artificial neurons that are linked to each other, with each link having an associated strength. When the output from one neuron passes to the subsequent neuron, it gets scaled by the strength of the link that connects the two neurons. This strength is called the **weight** of the link, and the weight is also just a numerical value similar to activation levels.\n",
    "\n",
    "As an example, consider a simple case with 2 neurons connected to each other through a link, and let the weight of the link be 'w'. For an input 'x' to neuron 1, the activation function squishes x into an number between 0 and 1, and this output of neuron 1 is then fed to neuron 2 through the link. The output is hence scaled by the weight of the link 'w', and becomes the input to neuron 2. Now the activation function of neuron 2 picks up this value and squishes it to \\[0, 1\\], which is the output of neuron 2.\n",
    "\n",
    "So the output becomes $ \\sigma_2( \\sigma_1(x) * w) ) $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large input: x=20 gives  0.982013789892\n",
      "Small input: x=-20 gives  0.500000002061\n"
     ]
    }
   ],
   "source": [
    "#Create a 2 neuron network, with 'x' as input and 'w' as weight of the link between the 2 neurons\n",
    "def simplest_neural_network(x, w):\n",
    "    return sigmoid(sigmoid(x)*w)\n",
    "\n",
    "print(\"Large input: x=20 gives \", simplest_neural_network(x = 20, w = 4)) \n",
    "print(\"Small input: x=-20 gives \", simplest_neural_network(x = -20, w = 4)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we're ready to start imagining slightly more complex neural network structures. To start with, we can imagine an easily comprehensible structure for the neural network (as opposed to imagining it as a messy interconnection of neurons like structure of our own brains). We do this by defining 3 layers of neurons in our network-\n",
    "\n",
    "1. an input layer of neurons (which can be thought of as our data inputs), \n",
    "2. a middle layer of neurons which are connected to the input layer through a set of weighted links (called the hidden layer), \n",
    "3. an output layer of neurons which are  connected to the middle layer through another set of linked weights. \n",
    "\n",
    "Here, we link every input neuron with every hidden layer neuron, and the same is true for the linking between the hidden layer and the output layer. This means that if there are 'm' input neurons and 'n' hidden layer neurons, the there are m * n links between the two layers. The outputs from the output layer form the result generated by the network.\n",
    "\n",
    "In the beginning, we can initialise all the weights to random numbers drawn from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />**Weight Matrix 1:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204708</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>-0.519439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.555730</td>\n",
       "      <td>1.965781</td>\n",
       "      <td>1.393406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -0.204708  0.478943 -0.519439\n",
       "1 -0.555730  1.965781  1.393406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**Weight Matrix 2:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092908</td>\n",
       "      <td>0.281746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.769023</td>\n",
       "      <td>1.246435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.007189</td>\n",
       "      <td>-1.296221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.092908  0.281746\n",
       "1  0.769023  1.246435\n",
       "2  1.007189 -1.296221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Neuralnetwork():\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        np.random.seed(12345)\n",
    "        # Let  us restrict ourselves to 1 hidden layer\n",
    "        self.input_layer = config[0]\n",
    "        self.output_layer = config[2]\n",
    "        self.hidden_layer = config[1]\n",
    "        \n",
    "        # Since we have 3 layers, we will have 2 sets of weights\n",
    "        # Initialise all these weights to 1 and biases to 0 - for now.\n",
    "        self.weight_list = [np.random.randn(self.input_layer, self.hidden_layer),\n",
    "                                 np.random.randn(self.hidden_layer, self.output_layer)]\n",
    "        return\n",
    "    \n",
    "# Create a test neural network with 2 inputs, 3 neurons in hidden layer and 2 outputs\n",
    "neuralnet = Neuralnetwork([2,3,2])\n",
    "#print(*neuralnet.weight_list, sep='\\n\\n')\n",
    "\n",
    "# Display the weight matrices\n",
    "display(Markdown(\"<br />**Weight Matrix 1:**\"),pd.DataFrame(neuralnet.weight_list[0]))\n",
    "display(Markdown(\"<br />**Weight Matrix 2:**\"), pd.DataFrame(neuralnet.weight_list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "Now that we have defined a complicated but well-organised neural network structure, how do we go about feeding inputs and obtaining outputs from the neural network? We will design a process by which the input layer accepts our inputs, and feeds it into successive layers through the links until we get the outputs from the output layer. \n",
    "\n",
    "Before we start the equations, note the following notations:\n",
    "\n",
    "| Notation | Dimensions | Description |\n",
    "|:---------------------:|:---:|:---:|\n",
    "| $X$ | $m \\times n$ |   Input matrix with $m$ rows and $n$ columns|\n",
    "| $Y$ | $m \\times p$ |   Expected output matrix with $m$ rows and $p$ columns|\n",
    "| $W^{(0)}$ | $n \\times h_1$ |   Weights connecting input layer to the hidden layer with $h_1$ neurons|\n",
    "| $Z^{(0)} = X W^{(0)}$ | $m \\times h_1$ |   The pre-activation vector for hidden layer|\n",
    "| $A^{(0)} = \\sigma(Z^{(0)})$ | $m \\times h_1$ |   The activation vector for hidden layer|\n",
    "| $W^{(1)}$ | $h_1 \\times p$ |   Weights connecting hidden layer to the output layer|\n",
    "| $Z^{(1)} = A^{(0)} W^{(1)}$ | $m \\times p$ |   The pre-activation vector for output layer|\n",
    "| $ \\hat{Y} = \\sigma(Z^{(1)})$ | $m \\times p$ |   The activation vector for hidden layer|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />**$X$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  0\n",
       "1  0  1\n",
       "2  0  0\n",
       "3  1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**$\\hat{Y}$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.851076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.842851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.787807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.851076\n",
       "1  0.775764\n",
       "2  0.842851\n",
       "3  0.787807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def feedforward(neuralnet, activation_vector):\n",
    "    # For a given input array, calculate the activations of each successive layer of neurons in the network.\n",
    "    # For this, multiply the activations of the first layer with the weights, and add the biases of the next layer; then apply\n",
    "    # the sigmoid function to obtain the activations of the next layer. Repeat this successively until we get to\n",
    "    # the output layer\n",
    "    i=0\n",
    "    z_vectors = []\n",
    "    activation_vectors = []\n",
    "    activation_vectors.append(activation_vector)\n",
    "    for weight in neuralnet.weight_list:\n",
    "        z_vector = np.dot(activation_vector, weight)\n",
    "        activation_vector = sigmoid(z_vector)\n",
    "        z_vectors.append(z_vector)\n",
    "        activation_vectors.append(activation_vector)\n",
    "\n",
    "    return activation_vectors\n",
    "            \n",
    "# We create a simple logical AND gate truth table as the input data to the neural network.\n",
    "x = np.array([ [1,0], [0,1], [0,0], [1,1]])  \n",
    "y = np.array([ [0], [0], [1],[0]])\n",
    "\n",
    "# Create a test neural network that accepts our defined data.\n",
    "neuralnet = Neuralnetwork([x.shape[1], 2, y.shape[1]])\n",
    "yhat = feedforward(neuralnet, x)\n",
    "\n",
    "# Print the activation vectors from each layer\n",
    "#print(*yhat, sep = \"\\n\\n\")\n",
    "\n",
    "# View output from feed forward step\n",
    "display(Markdown(\"<br />**$X$:**\"), pd.DataFrame(x))\n",
    "display(Markdown(\"<br />**$\\hat{Y}$:**\"), pd.DataFrame(yhat[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the output we obtain from the neural network is complete junk and is nowhere close to the expected AND gate output. This is because we chose the weights of the neural network quite randomly. What we need to do is to teach the neural network to give us what we want from it for any specfic input. We do this so that in the future, we can trust our network to instinctively knows what to give us as output everytime it sees a familiar input. \n",
    "\n",
    "How do we do this?\n",
    "\n",
    "### Back propagation\n",
    "\n",
    "We finally get to the difficult part of the neural network design, which involves training the network to give us what we want.\n",
    "\n",
    "First, the network needs to know it is doing a terrible job. We can do this by designing a cost metric, which the network uses to measure how badly it is performing. Next, it needs a way to adjust itself to give better estimates based on this cost metric.\n",
    "\n",
    "Let us start by defining the cost function. The cost can be thought of as a function of weights $W$, the input $X$, and expected output $Y$. \n",
    "\n",
    "Here again, statistics comes to our rescue. We can use the most commonly used cost metric- the sum of squared differences between our output and the actual outputs. To obtain the cost $C$, we take the square of the difference between each element in the $Y$ and $\\hat{Y}$ matrices (here, $\\hat{Y}$ is derived using $W$ and $X$). This can be represented using the Hadamard product between the difference of these matrices as represented below.\n",
    "\n",
    "\\begin{align}\n",
    "C & = f(W,X, Y) = \\sum_{i = 1, j = 1}^{m, p}(Y - \\hat{Y} ) \\circ (Y - \\hat{Y} )\n",
    "\\end{align}\n",
    "\n",
    "The cost function represents how far away we are from the expected output. Our objective is to minimise this cost function, and the  way we can do this is by adjusting the weights in the network, or the inputs themselves, or expected output, or the activations in the neurons themselves. Obviously, the inputs and expected outputs are already given and fixed. We do not have direct control over neuron activations as this is a fixed function- we only have control on its inputs (which is again dependent on the weights of the previous layers). Hence, the only real variable in our cost function are the weights. Our objective is to obtain minimum value of the cost function with respect to the weights in the network.\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla C = \\begin{bmatrix}\n",
    "\\frac{\\partial C}{\\partial{W^{(0)}}} \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(1)}}} \\\\\n",
    ".. \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Now, we can decompose the last term of the cost gradient vector as \n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} = \\frac{\\partial C}{\\partial{A^{(L)}}} \\cdot \\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}} \\cdot \\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}} }\n",
    "\\end{align}\n",
    "<br></br>\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L-1)}}} = \\frac{\\partial C}{\\partial{A^{(L-1)}}} \\cdot \\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}} \\cdot \\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}} }\n",
    "\\end{align}\n",
    "\n",
    "Here, we can substitute the following- \n",
    "\n",
    "| Derivatives                     | Description\n",
    "|:---------------------------------|----------|\n",
    "|$\\frac{\\partial{C}}{\\partial{A^{(L)}}}=-2(Y-\\hat{Y})$| Partial derivative of cost function with respect to $\\hat{Y}$  |\n",
    "|$\\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}}=\\sigma^{'}(Z^{(L)})$| Partial derivative of sigmoid function $\\sigma$    |\n",
    "|$\\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}}}=A^{(L-1)}$|Partial derivative of cost function with respect to previous layer of weights $W^{(L-1)}$|\n",
    "|$\\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}}=\\sigma^{'}(Z^{(L-1)})$|Partial derivative of sigmoid function $\\sigma$ for hidden layer neurons |\n",
    "|$\\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}}} = X$|Input Data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost =  16\n"
     ]
    }
   ],
   "source": [
    "# Cost function\n",
    "def cost(y, y_hat):\n",
    "    return np.sum( np.square(y - y_hat))\n",
    "\n",
    "print(\"Cost = \", cost(np.array([[5, 5], [5,5]]),np.array([[3,3], [3, 3]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_prime(self, y, yhat):\n",
    "        return -2*(y - yhat)\n",
    "\n",
    "def sigmoid_prime(z_vector):\n",
    "    return np.exp(np.multiply(z_vector, -1))/((1+np.exp(np.multiply(z_vector, -1)))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, output_vector):\n",
    "        \n",
    "    delta2 = np.multiply(self.cost_prime(output_vector, self.z_vectors[-1]), self.da_dz(self.z_vectors[-1]))\n",
    "    val2 = np.dot(np.transpose(self.activation_vectors[-2][0]), delta2[0])\n",
    "\n",
    "    delta1 = np.dot(delta2[0], np.transpose(self.weight_list[-1])) * self.da_dz(self.z_vectors[-2])\n",
    "    val1 = np.dot(np.transpose(tester.activation_vectors[0][0]), delta1[0])\n",
    "\n",
    "    self.weight_list[-1] = self.weight_list[-1] - val2\n",
    "    self.weight_list[-2] = self.weight_list[-2] - val1\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
