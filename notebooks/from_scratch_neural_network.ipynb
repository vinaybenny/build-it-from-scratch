{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch Series\n",
    "\n",
    "## 2. Artifical Neural  Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will attempt to formulate a intuitive understanding of how an artificial neural network works, along with building and explaining each component that forms part of the neural network. This article assumes that you are familiar with the basic concepts of machine learning and know a bit of elementary statistics and matrix algebra.\n",
    "\n",
    "We'll only use the base numpy and scipy libraries for creating a neural network from scratch. We use these liibraries to abstract out the details of basic operations like matrix multiplications & array handling, along with matplotlib for plotting  images.\n",
    "\n",
    "### First, the Neuron\n",
    "\n",
    "We can start off by defining what a single neuron is. A neuron can be thought of as an object that holds a number - and this number signifies how active or excited the object is.  Higher the value of this number, more active the neuron is, and vice versa. The neuron object has an input port which can be used to control how excited the neuron becomes. A function that takes this input and turns it into the activation level of the neuron is called the *activation function*. This activation or excitation level then becomes the output of that neuron.\n",
    "\n",
    "For now, let us assume that activation of a neuron is bounded within the range \\[0, 1\\] for any given input. So, for any numeric input to the neuron in the interval $[-\\infty, \\infty]$, we need to design an activation function that takes this input number and squishes it into a number between 0 and 1. We can think of the state 0 as the state of the neuron being completely inactive, and 1 as the state of neuron being fully activated. \n",
    "\n",
    "Let us design this activation function now. For now, the only requirement we have is that for any input, the function squishes the input into a number between 0 and 1. There is a very familiar function that maps an arbitrary input into \\[0, 1\\] interval - the **logit** function which is commonly used in logistic regression.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{-k(x - x_0)} }\n",
    "\\end{align}\n",
    "\n",
    "For keeping things simple, we use k=1, and x<sub>0</sub> = 0 as default for now, which makes the equation\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{x} } = \\sigma(x)\n",
    "\\end{align}\n",
    "\n",
    "We call this special case of the logit function as the **sigmoid**, and use a $\\sigma$ to represent it. Let us write some code to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.cell:nth-child(10) .output {\n",
       "    flex-direction: row;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mtl\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from IPython.display import display, HTML, Markdown # For printing nice looking tables from pandas dataframes\n",
    "from scipy import optimize # We will use this for mathematical optimisations later on\n",
    "\n",
    "# For printing dataframe tables side by side\n",
    "CSS = \"\"\"\n",
    "div.cell:nth-child(10) .output {\n",
    "    flex-direction: row;\n",
    "}\n",
    "\"\"\"\n",
    "HTML('<style>{}</style>'.format(CSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c92ca677f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-1.* x) )\n",
    "\n",
    "pyplot.plot(np.arange(-10, 10, 0.1), sigmoid(np.arange(-10, 10, 0.1)))\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "A neural network, as the name suggests, is a set of interconnected neurons. In biological neural networks, individual neurons are connected to each other through connections called synapses. Each synapse has a weight associated with it; this weight determines how strongly two neurons are connected to each other. A signal passed from one neuron to the next through this synapse is amplified (or attenuated) proportional to the weight of the synaptic connection.\n",
    "\n",
    "The artificial neural network we are constructing here can be be thought of as a bunch of artificial neurons that are linked to each other, with each link having an associated strength - just as in the biological counterpart. When the output from one neuron passes to the subsequent neuron, it gets scaled by the strength of the link that connects the two neurons. This strength is called the **weight** of the link, and the weight is also just a number, just like neuron activation levels.\n",
    "\n",
    "As an example, consider the simplest case with 2 neurons connected to each other through a link, and let the weight of the link be $w$. For an input $x$ to neuron 1, the activation function $\\sigma_1$ squishes $x$ into an number between 0 and 1, and this  is then passed to the synaptic link. The number is then scaled by the weight $w$ of the link, and becomes the input to neuron 2. Now the activation function $\\sigma_2$ of neuron 2 picks this value up and squishes it to \\[0, 1\\], which is the output of neuron 2.\n",
    "\n",
    "Hence the final output $y$ is given by  $ y = \\sigma_2( \\sigma_1(x) * w) ) $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large input: x=20 gives  0.982013789892\n",
      "Small input: x=-20 gives  0.500000002061\n"
     ]
    }
   ],
   "source": [
    "# Create a 2 neuron network, with 'x' as input and 'w' as weight of the link between the 2 neurons\n",
    "def simplest_neural_network(x, w):\n",
    "    return sigmoid(sigmoid(x)*w)\n",
    "\n",
    "print(\"Large input: x=20 gives \", simplest_neural_network(x = 20, w = 4)) \n",
    "print(\"Small input: x=-20 gives \", simplest_neural_network(x = -20, w = 4)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we're ready to start conceptualising slightly more complex neural network structures. Before we consider the neural network structure, consider the following problem- let's say we are supplied a set of $n$ numbers, and are asked to guess a function that takes these $n$ numbers as input and produces an number $y$. In mathematical terms, we are asked to guess function $f$ in the following equation-\n",
    "\n",
    "\\begin{align}\n",
    "y = f(x_1, x_2, ..., x_n)\n",
    "\\end{align}\n",
    "\n",
    "To make the function a bit easier to guess, let's say we are given several such input-output pairs that the function should produce. Suppose that we are given $m$ such pairs of inputs and outputs- in that case, we can rewrite the equation in matrix terms -\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & ... & x_{1n} \\\\\n",
    "x_{21} & x_{22} & ... & x_{2n} \\\\\n",
    ": & : & : & : \\\\\n",
    "x_{m1} & x_{m2} & ... & x_{mn} \\\\\n",
    "\\end{bmatrix} \n",
    "Z = \n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "y_{21} \\\\\n",
    ": \\\\\n",
    "y_{m1} \\\\\n",
    "\\end{bmatrix} \n",
    "\\end{align}\n",
    "\n",
    "which can be rewritten as\n",
    "\n",
    "\\begin{align}\n",
    "Y = XZ\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "So our job is to guess what $Z$ is. And how on earth would we go about guessing such a function? If you're familiar with statistics, then you'd know that there are plenty of parameter estimation techniques that you can choose from, like linear least squares estimators or maximum likelihood functions depending on the nature of the problem. And if you're a machine learning enthusiast, you'd probably pick techniques like SVMs or decision trees or neural networks. Both groups of techniques share the same mathematical foundations of optimisation, but each technique offers its own pros and cons. Since we are discussing neural networks in this article, let us try to design a neural network structure that solves the function estimation problem.\n",
    "\n",
    "To start with, we can imagine an easily comprehensible structure for the neural network (as opposed to imagining it as a messy interconnection of neurons like structure of our own brains). We are going to put 3 layers of neurons in our network-\n",
    "\n",
    "1. an input layer of neurons (this is where we feed in our data inputs), \n",
    "2. a middle layer of neurons which are connected to the input layer through a set of weighted links (called the hidden layer), \n",
    "3. an output layer of neurons which are connected to the middle layer through another set of linked weights.\n",
    "\n",
    "Obviously the input layer will have $n$ neurons, so as to accept the $n$ separate columns (or variables) in our input dataset. The choice of number of neurons in the hidden layer is arbitrary, and we will think of ways to choose this at a later stage. The output layer has one neuron for now, since we have only one column of output in the above example. However, note that there is no reason why the output has to be unidimensional; in fact we can have neural networks with multiple outputs, in which case the original problem becomes finding a function that relates input-output vector pairs.\n",
    "\n",
    "Why did we choose a network with three layers? Because obviously with just one layer of neurons there is no network that can be formed (we do not connect neurons within the same layer), and with two layers, what we get is a simplistic network that can only guess functions from the function space that are a linear combination of the input data. This fails to capture any non-linear relationships between the input and output.\n",
    "\n",
    "Here, we link every input neuron with every hidden layer neuron, and the same is true for the linking between the hidden layer and the output layer. This means that if there are $n$ input neurons and $h$ hidden layer neurons, the there are $n*h$ links between the two layers. Similarly, if there are $p$ output neurons, then there are $h*p$ connections between hidden layer and output layer. The outputs from the output layer form the result generated by the network. As stated earlier, we give each synaptic link a weight associated with it, which represents how strongly the two neurons are linked. As an initial guess for how strong these links should be, we can initialise all the weights to random numbers drawn from a standard normal distribution.\n",
    "\n",
    "Next, we code up the neural network structure and initialise the weights of the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />Weight Matrix between input and hidden layer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204708</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>-0.519439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.555730</td>\n",
       "      <td>1.965781</td>\n",
       "      <td>1.393406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -0.204708  0.478943 -0.519439\n",
       "1 -0.555730  1.965781  1.393406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />Weight Matrix between hidden and output layer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.769023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.092908\n",
       "1  0.281746\n",
       "2  0.769023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Neuralnetwork():\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        np.random.seed(12345)\n",
    "        \n",
    "        # For now, we restrict ourselves to 1 hidden layer for simplicity's sake\n",
    "        self.input_layer_size = config[0]\n",
    "        self.output_layer_size = config[2]\n",
    "        self.hidden_layer_size = config[1]\n",
    "        \n",
    "        # Since we have 3 layers, we will have 2 sets of weights\n",
    "        # Initialise all these weights to 1 and biases to 0 - for now.\n",
    "        self.weight_list = [np.random.randn(self.input_layer_size, self.hidden_layer_size),\n",
    "                                 np.random.randn(self.hidden_layer_size, self.output_layer_size)]\n",
    "        return\n",
    "    \n",
    "# Create a test neural network with 2 inputs, 3 neurons in hidden layer and 1 output\n",
    "neuralnet = Neuralnetwork([2,3,1])\n",
    "#print(*neuralnet.weight_list, sep='\\n\\n')\n",
    "\n",
    "# Display the weight matrices\n",
    "display(Markdown(\"<br />Weight Matrix between input and hidden layer:\"),pd.DataFrame(neuralnet.weight_list[0]))\n",
    "display(Markdown(\"<br />Weight Matrix between hidden and output layer:\"), pd.DataFrame(neuralnet.weight_list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "Now that we have defined a complicated but well-organised neural network structure, how do we go about feeding inputs and obtaining outputs from the neural network? We will design a process by which the input layer accepts our inputs, and feeds it into successive layers through the links until we get the outputs from the output layer. It may look mathematically cumbersome to take each input row from the input matrix, multiply with the weights of each link it passes through, then propagating these results forward to next layer of neurons, applying the activation functions, and repeating the process until we reach the output layer; fortunately, matrix algebra comes to our aid.\n",
    "\n",
    "We can implement this forward pass of inputs through the neural network layers as a set of successive matrix multiplications, interspersed with the application of activation functions on the output of each layer. The sequence of operations will be clear from the following table:\n",
    "\n",
    "| Notation | Dimensions | Description |\n",
    "|:---------------------:|:---:|:---:|\n",
    "| $X$ | $m \\times n$ |   Input matrix with $m$ rows and $n$ columns|\n",
    "| $Y$ | $m \\times p$ |   The given output matrix with $m$ rows and $p$ columns|\n",
    "| $W^{(0)}$ | $n \\times h_1$ |   Weights connecting input layer to the hidden layer with $h_1$ neurons|\n",
    "| $Z^{(0)} = X W^{(0)}$ | $m \\times h_1$ |   The signals coming into the hidden layer neurons|\n",
    "| $A^{(0)} = \\sigma(Z^{(0)})$ | $m \\times h_1$ |   The set of activation vectors for hidden layer|\n",
    "| $W^{(1)}$ | $h_1 \\times p$ |   Weights connecting hidden layer to the output layer with $p$ columns|\n",
    "| $Z^{(1)} = A^{(0)} W^{(1)}$ | $m \\times p$ |   The signals coming into the output layer|\n",
    "| $ \\hat{Y} = \\sigma(Z^{(1)})$ | $m \\times p$ |   The set of activation vectors for output layer which form the output|\n",
    "\n",
    "\n",
    "\n",
    "Next we implement this forward propagation in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />**$X$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  0\n",
       "1  0  1\n",
       "2  1  0\n",
       "3  1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**$\\hat{Y}$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.842851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.851076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.787807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.842851\n",
       "1  0.775764\n",
       "2  0.851076\n",
       "3  0.787807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**${Y}$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forwardprop(neuralnet, X):\n",
    "    # For a given input array, calculate the activations of each successive layer of neurons in the network.\n",
    "    # For this, multiply the activations of the first layer with the weights, and add the biases of the next layer; then apply\n",
    "    # the sigmoid function to obtain the activations of the next layer. Repeat this successively until we get to\n",
    "    # the output layer\n",
    "    i=0\n",
    "    activation_vector = X\n",
    "    neuralnet.z_vectors = [ [], [] ]\n",
    "    neuralnet.activation_vectors = [ [], [], [] ]\n",
    "    neuralnet.activation_vectors[i].append(activation_vector)\n",
    "    \n",
    "    for weight in neuralnet.weight_list:\n",
    "        z_vector = np.dot(activation_vector, weight)\n",
    "        activation_vector = sigmoid(z_vector)\n",
    "        neuralnet.z_vectors[i].append(z_vector)\n",
    "        neuralnet.activation_vectors[i+1].append(activation_vector)            \n",
    "        i = i+1\n",
    "\n",
    "    yhat = activation_vector\n",
    "    return yhat\n",
    "\n",
    "# We create a simple logical XOR gate truth table as the input data to the neural network.\n",
    "x = np.array([ [0,0], [0,1], [1,0], [1,1]])  \n",
    "y = np.array([ [0], [1], [1], [0]])\n",
    "\n",
    "# Create a test neural network that accepts our defined data.\n",
    "neuralnet = Neuralnetwork([x.shape[1], 2, y.shape[1]])\n",
    "yhat = forwardprop(neuralnet, x)\n",
    "\n",
    "# Print the activation vectors from each layer\n",
    "#print(*yhat, sep = \"\\n\\n\")\n",
    "\n",
    "# View output from feed forward step\n",
    "display(Markdown(\"<br />**$X$:**\"), pd.DataFrame(x))\n",
    "display(Markdown(\"<br />**$\\hat{Y}$:**\"), pd.DataFrame(yhat))\n",
    "display(Markdown(\"<br />**${Y}$:**\"), pd.DataFrame(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the output vector $\\hat{Y}$ we obtain from the neural network is complete junk and is nowhere close to the actual output $Y$ vector. This is because we chose the weights of the neural network quite randomly. What we need to do is to train the neural network to give us our expected output for a specfic input. In other words, we want the neural network to guess the underlying function that relates each row in the input matrix $X$ to the corresponding output in vector $Y$. To do this, we need to show the neural network several instances of the inputs and expected outputs, based on which the neural network automatically adjusts its weights to give us an output that is as close as possible to the expected output.\n",
    "\n",
    "How do we do this? This is where the backpropagation algorithm comes in.\n",
    "\n",
    "### Back propagation\n",
    "\n",
    "We finally get to the difficult part of the neural network design, which involves training the network to give us the output we want.\n",
    "\n",
    "First, the network needs to know it is doing a terrible job. We can do this by designing a cost metric, which the network uses to measure how badly it is performing. Next, it needs a way to adjust itself to give better estimates based on this cost metric.\n",
    "\n",
    "Let us start by defining the cost function. Here again, statistics comes to our rescue. We pick the most commonly used cost metric in statistics- the sum of squared differences between the obtained output and the expected output. To obtain the cost $C$, we take the square of the difference between each corresponding elements in the expected output $Y$ and the obtained output $\\hat{Y}$. As stated earlier, there is no restriction on $Y$ and $\\hat{Y}$ to be vectors- these are special cases when there is only one output neuron. There can be multiple output neurons in a neural network, and in such cases $Y$ and $\\hat{Y}$ can be considered as matrices with each column representing each output neuron activation. In general terms, we can consider $Y$ and $\\hat{Y}$ as output matrices (in cases where there are more than one output neuron).\n",
    "\n",
    "Hence the cost function can be represented as the trace (sum of main diagonal) of the square of the error matrix as follows-\n",
    "\n",
    "\\begin{align}\n",
    "C & = tr( (Y - \\hat{Y} )^T (Y - \\hat{Y} ) )\n",
    "\\end{align}\n",
    "\n",
    "The cost function represents how far away we are from the expected output. Our objective is to minimise this cost function, and there are 3 ways we can do this- either by adjusting $X$, or $Y$, or the activations of the neurons in the network. Obviously, the inputs $X$ and expected outputs $Y$ are already given and fixed- in fact we are trying to adjust the network to get the expected output given the input. All we have control over are the neuron activation levels. And since the neuron activation function $\\sigma$ is fixed, the only remaining component that can be changed are the synaptic weights between the neurons of different layers. Hence, the only real variable in our cost function are the weights. Our objective is to obtain minimum value of the cost function with respect to the weights in the network.\n",
    "\n",
    "\n",
    "**From here, revisit the text**\n",
    "\n",
    "In mathematical terms, this can be written as \n",
    "\\begin{align}\n",
    "C & = tr( (Y - \\hat{Y} )^T (Y - \\hat{Y} ) ) = f(X, Y, W)\n",
    "\\end{align}\n",
    "\n",
    "Since $X$ and $Y$ are fixed and constant in this context, this equation becomes\n",
    "\\begin{align}\n",
    "C = f(W)\n",
    "\\end{align}\n",
    "\n",
    "Minimization of a function involves knowing the gradient of that function. Since we have a function with several weights, our cost gradient vector becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla C = \\begin{bmatrix}\n",
    "\\frac{\\partial C}{\\partial{W^{(0)}}} \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(1)}}} \\\\\n",
    ".. \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We have only 2 sets of weights- the ones between input and hidden layer, and the one between hidden and otput layers. Using the chain rule of differentiation, we can decompose the cost gradient vector as \n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} = \\frac{\\partial C}{\\partial{A^{(L)}}} \\cdot \\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}} \\cdot \\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}} }\n",
    "\\end{align}\n",
    "<br></br>\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L-1)}}} = \\frac{\\partial C}{\\partial{A^{(L-1)}}} \\cdot \\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}} \\cdot \\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}} }\n",
    "\\end{align}\n",
    "\n",
    "Here, we can substitute the following- \n",
    "\n",
    "| Derivatives                     | Description|\n",
    "|:--------------------------------|:-----------|\n",
    "|$\\frac{\\partial{C}}{\\partial{A^{(L)}}}=-2(Y-\\hat{Y})$| Partial derivative of cost function with respect to $\\hat{Y}$  |\n",
    "|$\\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}}=\\sigma^{'}(Z^{(L)})$| Partial derivative of sigmoid function $\\sigma$    |\n",
    "|$\\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}}}=A^{(L-1)}$|Partial derivative of cost function with respect to previous layer of weights $W^{(L-1)}$|\n",
    "|$\\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}}=\\sigma^{'}(Z^{(L-1)})$|Partial derivative of sigmoid function $\\sigma$ for hidden layer neurons |\n",
    "|$\\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}}} = X$|Input Data |\n",
    "\n",
    "\n",
    "All that remains is to code up the cost function, its gradient function, and the backpropagation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost(y, yhat):\n",
    "    return np.square(y - yhat)\n",
    "\n",
    "def cost_prime(y, yhat):\n",
    "    return -1*(y - yhat)\n",
    "\n",
    "def sigmoid_prime(z_vector):\n",
    "    return np.exp(np.multiply(z_vector, -1))/((1+np.exp(np.multiply(z_vector, -1)))**2)\n",
    "\n",
    "def backprop(neuralnet, output_vector):\n",
    "\n",
    "    delta2 = np.multiply(cost_prime(output_vector, neuralnet.z_vectors[-1]), sigmoid_prime(neuralnet.z_vectors[-1]))\n",
    "    val2 = np.dot(np.transpose(neuralnet.activation_vectors[-2][0]), delta2[0])\n",
    "\n",
    "    delta1 = np.dot(delta2[0], np.transpose(neuralnet.weight_list[-1])) * sigmoid_prime(neuralnet.z_vectors[-2])\n",
    "    val1 = np.dot(np.transpose(neuralnet.activation_vectors[0][0]), delta1[0])\n",
    "\n",
    "    return np.concatenate((val1.ravel(), val2.ravel()))\n",
    "\n",
    "def costwrapper(neuralnet, params, X, y):\n",
    "    forwardprop(neuralnet, X)\n",
    "    cost = neuralnet.cost(y, neuralnet.z_vectors[-1])\n",
    "    gradient = backprop(neuralnet, y)\n",
    "    return cost, gradient\n",
    "\n",
    "def train(neuralnet, X, y):\n",
    "    optimize.minimize(costwrapper, np.concatenate((neuralnet.weight_list[0].ravel(), neuralnet.weight_list[1].ravel())),\n",
    "                      jac = True, method = 'BFGS', args = (neuralnet, X, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'z_vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-857996d94333>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.82\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.93\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtester\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtester\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-d5177438bc42>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(neuralnet, X, y)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneuralnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     optimize.minimize(costwrapper, np.concatenate((neuralnet.weight_list[0].ravel(), neuralnet.weight_list[1].ravel())),\n\u001b[1;32m---> 29\u001b[1;33m                       jac = True, method = 'BFGS', args = (neuralnet, X, y), options = {'disp': True, 'maxiter':200})\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[0mgrad_calls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyfprime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m     \u001b[0mgfk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyfprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mderivative\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-d5177438bc42>\u001b[0m in \u001b[0;36mcostwrapper\u001b[1;34m(neuralnet, params, X, y)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcostwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneuralnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mforwardprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneuralnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneuralnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-34d136612495>\u001b[0m in \u001b[0;36mforwardprop\u001b[1;34m(neuralnet, X)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mactivation_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'z_vectors'"
     ]
    }
   ],
   "source": [
    "#Test code  \n",
    "x = np.array([ [3,5], [5,1], [10,2]])  \n",
    "y = np.array([ [0.75], [0.82], [0.93]])\n",
    "tester = Neuralnetwork([x.shape[1], 3, y.shape[1]])\n",
    "train(tester, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057,\n",
       "        1.39340583,  0.09290788,  0.28174615,  0.76902257])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((tester.weight_list[0].ravel(), tester.weight_list[1].ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
