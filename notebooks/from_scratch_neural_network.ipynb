{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build it from Scratch Series\n",
    "\n",
    "## 2. Artifical Neural  Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we will attempt to understand how the (perhaps rightly) glorified artificial neural networks work. There is a lot of material out there that describe and implement a neural network in a mathematically and functionally rigourous fashion, but this isn't the focus of our article. Our attempt is to formulate a layman's understanding of what happens inside a neural network and describe how it works. We will focus on gradually building up each component of the neural network in a naive and simplistic fashion, to keep the basics as clear as possible.\n",
    "\n",
    "This article assumes that you are familiar with the basic concepts of machine learning and have heard of neural networks before- we will not go into the oft-repeated history and inspiration behind neural networks. We also assume you know a bit of elementary statistics and matrix algebra.\n",
    "\n",
    "We will be using Scipy (python 3) to code up the neural network, and will only use the base numpy and scipy libraries for creating a neural network from scratch. We use these libraries to abstract out the details of basic operations like matrix multiplications & array handling, along with matplotlib for plotting graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, The Neuron\n",
    "\n",
    "Obviously, the name 'neural network' suggests that it is some sort of a network of neural things; and yes, these things are called neurons.\n",
    "\n",
    "We can start off by defining what a single neuron is. A neuron can be thought of as an object that holds a number. The magnitude of the number suggests how active or excited the neuron is, much like the volume level of a stereo. Higher the value of this number, more active the neuron is, and vice versa. \n",
    "\n",
    "And just like there is a knob on the stereo that controls the volume, the neuron also has an input port which can be used to control how active the neuron becomes. For any input signal supplied through this input port, a mathematical function converts this input into the activation level of the neuron. This function is rather unimaginatively called the *activation function* of the neuron. The activation or excitation level can be considered as the output from the neuron.\n",
    "\n",
    "### Activation function\n",
    "For now, let us assume that activation of a neuron is bounded within the range $[0, 1]$ for any given input signal. We can think of 0 as the state of the neuron being completely inactive, and 1 as the state of neuron being fully active. So, for any numeric input supplied to the neuron in the interval $[-\\infty, \\infty]$, we need to design an activation function that takes this input number and squishes it into a number between 0 and 1. \n",
    "\n",
    "How do we go about designing such an activation function? For now, the only requirement we have is that for any input, the function squishes the input into a number between 0 and 1. For those who know a bit of statistics, there is a very familiar function that maps an arbitrary numeric input into \\[0, 1\\] interval - the **logit** function which is commonly used in logistic regression. The logit function is given by\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{-k(x - x_0)} }\\notag\n",
    "\\end{align}\n",
    "\n",
    "For keeping things simple, we use k=1, and x<sub>0</sub> = 0 as default for now, which makes the equation\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\frac{1}{1 + e^{-x} } = \\sigma(x)\\notag\n",
    "\\end{align}\n",
    "\n",
    "We call this special case of the logit function as the **sigmoid**, and use a $\\sigma$ to represent it. Let us write some code to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x220b8a23eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import matplotlib as mtl\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from IPython.display import display, HTML, Markdown # For printing nice looking tables from pandas dataframes\n",
    "\n",
    "# For printing dataframe tables side by side\n",
    "CSS = \"\"\"\n",
    "div.cell:nth-child(10) .output {\n",
    "    flex-direction: row;\n",
    "}\n",
    "\"\"\"\n",
    "HTML('<style>{}</style>'.format(CSS))\n",
    "\n",
    "\n",
    "# Create the activation function for the neuron\n",
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-1.* x) )\n",
    "\n",
    "\n",
    "# Plot the sigmoid function\n",
    "pyplot.plot(np.arange(-10, 10, 0.1), sigmoid(np.arange(-10, 10, 0.1)))\n",
    "pyplot.grid(True)\n",
    "pyplot.title('The Sigmoid')\n",
    "pyplot.xlabel('x')\n",
    "pyplot.ylabel('y')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network\n",
    "\n",
    "A neural network, as the name suggests, is a set of interconnected neurons. In biological neural networks, individual neuron cells are connected to each other through connections called synapses. Each synapse has a weight associated with it; this weight determines how strongly bound two neurons are. A signal passed from one neuron to the next through this synapse is amplified (or attenuated) proportional to the strength of this synaptic connection.\n",
    "\n",
    "Just as in the biological counterpart, the artificial neural network we are constructing here can be be thought of as a bunch of artificial neurons that are connected to each other, with each link having an associated strength. When the output from one neuron passes to the subsequent neuron, it gets scaled by the strength of the link that connects the two neurons. This strength is called the **weight** of the link, and the weight is also a numerical quantity, just like neuron activation level.\n",
    "\n",
    "As an example, consider the simplest possible neural network with 2 neurons connected to each other through a link. Let the weight of the link be $w$. For an input $x$ to neuron A, the activation function $\\sigma_A$ squishes $x$ into an number between 0 and 1, and this  is then passed to the synaptic link. The number is then scaled by the weight $w$ of the link, and becomes the input to neuron B. Now the activation function $\\sigma_B$ of neuron B picks this value up and squishes it to \\[0, 1\\], which is the output of neuron B.\n",
    "\n",
    "Hence the final output $y$ is given by  $ y = \\sigma_B( \\sigma_A(x) * w) ) $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large input: x=20 gives  0.982013789892\n",
      "Small input: x=-20 gives  0.500000002061\n"
     ]
    }
   ],
   "source": [
    "# Create a 2 neuron network, with 'x' as input and 'w' as weight of the link between the 2 neurons\n",
    "def simplest_neural_network(x, w):\n",
    "    return sigmoid(sigmoid(x)*w)\n",
    "\n",
    "print(\"Large input: x=20 gives \", simplest_neural_network(x = 20, w = 4)) \n",
    "print(\"Small input: x=-20 gives \", simplest_neural_network(x = -20, w = 4)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "Before we consider more complex neural network structures, contemplate the following problem- let's say we are supplied a set of $n$ numbers, and are asked to design a function that takes these $n$ numbers as input and produces a given output $y$. In mathematical terms, we are asked to guess function $F$ in the following equation-\n",
    "\n",
    "\\begin{align}\n",
    "y = F(x_1, x_2, ..., x_n)\n",
    "\\end{align}\n",
    "\n",
    "To make the function a bit easier to guess, let's say we are given several such input-output pairs that the function should produce. Suppose that we are given $m$ such pairs of inputs and outputs- in that case, we can rewrite the equation in matrix terms -\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\\\\n",
    "y_{21} \\\\\n",
    ": \\\\\n",
    "y_{m1} \\\\\n",
    "\\end{bmatrix}\n",
    "= F(\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & ... & x_{1n} \\\\\n",
    "x_{21} & x_{22} & ... & x_{2n} \\\\\n",
    ": & : & : & : \\\\\n",
    "x_{m1} & x_{m2} & ... & x_{mn} \\\\\n",
    "\\end{bmatrix} \n",
    ")\n",
    "\\end{align}\n",
    "\n",
    "which can be rewritten as\n",
    "\n",
    "\\begin{align}\n",
    "Y = F(X)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$F$ is a function that acts on each row of the input matrix $X$ and produces each element of the output $Y$ vector. And how on earth would we go about guessing the parameters of this function? \n",
    "\n",
    "If you're familiar with statistics, then you'd know that there are plenty of parameter estimation techniques that you can choose from, like linear least squares estimators or maximum likelihood functions. And if you're a machine learning enthusiast, you'd probably pick techniques like SVMs or decision trees or neural networks. Both groups of techniques share the same mathematical foundations of optimisation, albeit with different assumptions. And each technique comes with its own assumptions and pros and cons. The point of saying all this is that the neural network is just such a technique that solves the problem that we defined here. So, let us try to design a neural network structure that solves the function estimation problem.\n",
    "\n",
    "To start with, we can imagine an easily comprehensible structure for the neural network (as opposed to imagining it as a messy interconnection of neurons like structure of our own brains). We are going to put 3 layers of neurons in our network-\n",
    "\n",
    "1. an input layer of neurons (this is where we feed in our data inputs), \n",
    "2. a middle layer of neurons which are connected to the input layer through a set of weighted links (we call this the hidden layer), \n",
    "3. an output layer of neurons which are connected to the middle layer through another set of weighted links.\n",
    "\n",
    "Obviously the input layer will have $n$ neurons, so as to accept the $n$ separate columns (or variables) in our input dataset as defined in the problem. In the hidden layer, we do not have any reasoning at this moment to choose the number of neurons, so we'll choose an arbitrary value. We will think of better ways to choose this more wisely at a later stage. The output layer has one neuron for now, since we have only one column of output in the problem definition above. However, note that there is no reason why the output has to be unidimensional; in fact we can have neural networks with multiple outputs, in which case the original problem becomes finding a function that relates input-output vector pairs.\n",
    "\n",
    "Why did we choose a network with three layers? Because obviously with just one layer of neurons there is no network that can be formed (we do not connect neurons within the same layer). And with two layers, what we get is a very simplistic network that can only guess functions which are a linear combination of the input data. This is because there is only one layer of weights between input and output layer, and all possible functions involve multiplying the input data matrix with a vector of weights. While there is nothing wrong with such a neural network, we are severly limited in the functions to choose from. Such neural networks fail to capture any non-linear relationships that may exist between the input and output. As a result, we stick with 3 layers of neurons in our neural network.\n",
    "\n",
    "For our neural network, we link every input neuron with every hidden layer neuron, and similarly every hidden layer neuron with the output layer. This means that if there are $n$ input neurons and $h$ hidden layer neurons, the there are $n*h$ links between the two layers. Similarly, if there are $p$ output neurons, then there are $h*p$ links between hidden layer and output layer. The outputs from the output layer form the result generated by the network. As stated earlier, each of these links have a weight associated with it, which represents how strongly the two neurons are linked. \n",
    "\n",
    "We have no reason to believe that the weights of these links should be anything specific, so we make a random initial guess for how strong these links should be. All the weights are initialised to random numbers drawn from a standard normal distribution.\n",
    "\n",
    "Next, we code up this neural network structure and initialise the weights of the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />Weight Matrix between input and hidden layer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204708</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>-0.519439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.555730</td>\n",
       "      <td>1.965781</td>\n",
       "      <td>1.393406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -0.204708  0.478943 -0.519439\n",
       "1 -0.555730  1.965781  1.393406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />Weight Matrix between hidden and output layer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.769023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.092908\n",
       "1  0.281746\n",
       "2  0.769023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Neuralnetwork():\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        np.random.seed(12345)\n",
    "        \n",
    "        # For now, we restrict ourselves to 1 hidden layer for simplicity's sake\n",
    "        self.input_layer_size = config[0]\n",
    "        self.hidden_layer_size = config[1]\n",
    "        self.output_layer_size = config[2]\n",
    "        \n",
    "        # Since we have 3 layers, we will have 2 sets of weights\n",
    "        # Initialise all these weights to random numbers for now.\n",
    "        self.weight_list = [np.random.randn(self.input_layer_size, self.hidden_layer_size),\n",
    "                                 np.random.randn(self.hidden_layer_size, self.output_layer_size)]\n",
    "        return\n",
    "    \n",
    "# Create a test neural network with 2 inputs, 3 neurons in hidden layer and 1 output\n",
    "neuralnet = Neuralnetwork([2,3,1])\n",
    "#print(*neuralnet.weight_list, sep='\\n\\n')\n",
    "\n",
    "# Display the weight matrices\n",
    "display(Markdown(\"<br />Weight Matrix between input and hidden layer:\"),pd.DataFrame(neuralnet.weight_list[0]))\n",
    "display(Markdown(\"<br />Weight Matrix between hidden and output layer:\"), pd.DataFrame(neuralnet.weight_list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "Now that we have defined a complicated but well-organised neural network structure, how do we go about feeding inputs and obtaining outputs from the network? We will have to design a process by which the input layer accepts our inputs, and feeds it into successive layers through the links until we get the outputs from the output layer. It may look mathematically cumbersome, but the steps required are- \n",
    "* take each input row from the input matrix, \n",
    "* multiply with the weights of each link it passes through, \n",
    "* then propagate these results forward to the hidden layer of neurons, \n",
    "* sum up the inputs to each neuron in this layer, and apply the activation functions,\n",
    "* multiply the output from each neuron with the weights of each link it passes through,\n",
    "* then propagate these results forward to the output layer of neurons, \n",
    "* sum up the inputs to each neuron in this layer, and apply the activation functions.\n",
    "* Get output from the output layer.\n",
    "\n",
    "Quite fortunately, matrix algebra comes to our aid. We can implement this forward pass of inputs through the neural network layers as a set of successive matrix multiplications, interspersed with the application of activation functions on the output of each layer. The sequence of operations will be clear from the following table:\n",
    "\n",
    "| Notation | Dimensions | Description |\n",
    "|:---------------------:|:---:|:---:|\n",
    "| $X$ | $m \\times n$ |   Input matrix with $m$ rows and $n$ columns|\n",
    "| $Y$ | $m \\times p$ |   The given output matrix with $m$ rows and $p$ columns|\n",
    "| $W^{(0)}$ | $n \\times h_1$ |   Weight matrix connecting input layer to the hidden layer with $h_1$ neurons|\n",
    "| $Z^{(0)} = X W^{(0)}$ | $m \\times h_1$ |   The signals coming into the hidden layer neurons|\n",
    "| $A^{(0)} = \\sigma(Z^{(0)})$ | $m \\times h_1$ |   The set of activation vectors for hidden layer|\n",
    "| $W^{(1)}$ | $h_1 \\times p$ |   Weights connecting hidden layer to the output layer with $p$ columns|\n",
    "| $Z^{(1)} = A^{(0)} W^{(1)}$ | $m \\times p$ |   The signals coming into the output layer|\n",
    "| $ \\hat{Y} = \\sigma(Z^{(1)})$ | $m \\times p$ |   The set of activation vectors for output layer which form the output|\n",
    "\n",
    "\n",
    "You can verify by hand that these matrix operations correspond to each step that we defined for the forward propagation of inputs. Next we implement this forward propagation in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br />**$X$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  0\n",
       "1  0  1\n",
       "2  1  0\n",
       "3  1  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**$\\hat{Y}$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.842851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.851076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.787807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.842851\n",
       "1  0.775764\n",
       "2  0.851076\n",
       "3  0.787807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br />**${Y}$:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forwardprop(neuralnet, X):\n",
    "    # For a given input array, calculate the activations of each successive layer of neurons in the network.\n",
    "    # For this, multiply the activations of the first layer with the weights, and add the biases of the next layer; then apply\n",
    "    # the sigmoid function to obtain the activations of the next layer. Repeat this successively until we get to\n",
    "    # the output layer\n",
    "    i=0\n",
    "    activation_vector = X\n",
    "    neuralnet.z_vectors = [ [], [] ]\n",
    "    neuralnet.activation_vectors = [ [], [], [] ]\n",
    "    neuralnet.activation_vectors[i].append(activation_vector)\n",
    "    \n",
    "    for weight in neuralnet.weight_list:\n",
    "        z_vector = np.dot(activation_vector, weight)\n",
    "        activation_vector = sigmoid(z_vector)\n",
    "        neuralnet.z_vectors[i].append(z_vector)\n",
    "        neuralnet.activation_vectors[i+1].append(activation_vector)            \n",
    "        i = i+1\n",
    "\n",
    "    yhat = activation_vector\n",
    "    return yhat\n",
    "\n",
    "# We create a simple logical XOR gate truth table as the input data to the neural network.\n",
    "x = np.array([ [0,0], [0,1], [1,0], [1,1]])  \n",
    "y = np.array([ [0], [1], [1], [0]])\n",
    "\n",
    "# Create a test neural network that accepts our defined data.\n",
    "neuralnet = Neuralnetwork([x.shape[1], 2, y.shape[1]])\n",
    "yhat = forwardprop(neuralnet, x)\n",
    "\n",
    "# View output from forward propagation step\n",
    "display(Markdown(\"<br />**$X$:**\"), pd.DataFrame(x))\n",
    "display(Markdown(\"<br />**$\\hat{Y}$:**\"), pd.DataFrame(yhat))\n",
    "display(Markdown(\"<br />**${Y}$:**\"), pd.DataFrame(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the output vector $\\hat{Y}$ we obtain from the neural network is complete junk and is nowhere close to the actual output $Y$ vector. This is because we chose the weights of the neural network quite randomly. What we need to do is to think of a way to adjust the weights so that the neural network gives us our expected output for a specfic input. In other words, we want the neural network to guess the underlying function that relates each row in the input matrix $X$ to the corresponding output in vector $Y$ by adjusting the weights. \n",
    "\n",
    "How do we do this? This is where the backpropagation algorithm comes in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation\n",
    "\n",
    "We finally get to the difficult part of the neural network design, which involves defining a way by which the network self corrects to give us the output we want.\n",
    "\n",
    "First, the network needs to know it is doing a terrible job. We can do this by designing a cost metric, which the network uses to measure how badly it is performing. Next, it needs a way to adjust itself to give better estimates based on this cost metric.\n",
    "\n",
    "Let us start by defining the cost function. Here again, statistics comes to our rescue. We pick the most commonly used cost metric in statistics- the sum of squared differences between the obtained output and the expected output. To obtain the cost $C$, we take the square of the difference between each corresponding elements in the expected output $Y$ and the obtained output $\\hat{Y}$. As stated earlier, there is no restriction on $Y$ and $\\hat{Y}$ to be vectors- these are special cases when there is only one output neuron. There can be multiple output neurons in a neural network, and in such cases $Y$ and $\\hat{Y}$ can be considered as matrices with each column representing each output neuron activation. In general terms, we can consider $Y$ and $\\hat{Y}$ as output matrices (in cases where there are more than one output neuron).\n",
    "\n",
    "In mathematical terms, the cost function can be represented as the trace of the square of the error matrix. The diagonal of the matrix consists of the square of each element in $Y-\\hat{Y}$, and the trace gives us its sum.\n",
    "\n",
    "\\begin{align}\n",
    "C & = tr( (Y - \\hat{Y} )^T (Y - \\hat{Y} ) )\n",
    "\\end{align}\n",
    "\n",
    "The cost function represents how far away we are from the expected output. Our objective is to minimise this cost function, and we must look for ways we can do this. On examining the equation, we find that while $Y$ is fixed and given to us, $\\hat{Y}$ is calculated through a series of matrix operations in the forward propagation algorithm using the input data $X$, the intermediate weight vectors between the neuron layers $W^{(i)}$ and the activation function $\\sigma$. But $X$ is fixed, and we have already decided that our activation function is the sigmoid, which leaves us with one choice to minimize cost- modifying the synaptic weights.\n",
    "\n",
    "Our objective hence becomes minimising the cost function with respect to the synaptic weights in the network. Minimization of a function involves knowing the gradient of that function with respect to the independent variables. Since we have a function with several weights, our cost gradient vector becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla C = \\begin{bmatrix}\n",
    "\\frac{\\partial C}{\\partial{W^{(0)}}} \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(1)}}} \\\\\n",
    ".. \\\\\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} \n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In our case,we have 2 sets of weights- the ones between input and hidden layer, and the one between hidden and output layers. Using the chain rule of differentiation, we can decompose the cost gradient vector as \n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} = \\frac{\\partial C}{\\partial{A^{(L)}}} \\cdot \\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}} \\cdot \\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}} }\n",
    "\\end{align}\n",
    "<br></br>\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L-1)}}} = \\frac{\\partial C}{\\partial{A^{(L)}}} \\cdot \\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}} \\cdot \\frac{\\partial{Z^{(L)}}}{\\partial{A^{(L-1)}}} \\cdot \\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}} \\cdot \\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}} }\n",
    "\\end{align}\n",
    "\n",
    "Here, we can substitute each of these partial derivatives using the following- \n",
    "\n",
    "| Derivatives                     | Description|\n",
    "|:--------------------------------|:-----------|\n",
    "|$\\frac{\\partial{C}}{\\partial{A^{(L)}}}=-2(Y-\\hat{Y})$| Partial derivative of cost function with respect to $\\hat{Y}$  |\n",
    "|$\\frac{\\partial{A^{(L)}}}{\\partial{Z^{(L)}}}=\\sigma^{'}(Z^{(L)})$| Partial derivative of sigmoid activation function $\\sigma$ with respect to pre-activation vector $Z^{(L)}$  |\n",
    "|$\\frac{\\partial{Z^{(L)}}}{\\partial{W^{(L)}}}=A^{(L-1)}$|Partial derivative of pre-activation vector $Z^{(L)}$ with respect to weights $W^{(L)}$|\n",
    "|$\\frac{\\partial{Z^{(L)}}}{\\partial{A^{(L-1)}}}=W^{(L)}$|Partial derivative of pre-activation vector $Z^{(L)}$ with respect to previous hidden layer activations $A^{(L-1)}$|\n",
    "|$\\frac{\\partial{A^{(L-1)}}}{\\partial{Z^{(L-1)}}}=\\sigma^{'}(Z^{(L-1)})$|Partial derivative of sigmoid function $\\sigma$ with respect to pre-activation vector $Z^{(L-1)}$ for hidden layer neurons |\n",
    "|$\\frac{\\partial{Z^{(L-1)}}}{\\partial{W^{(L-1)}}} = X$|Input Data |\n",
    "\n",
    "Based on these substitutions, we can rewrite the above equations as\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L)}}} = -2(Y-\\hat{Y}) \\cdot \\sigma^{'}(Z^{(L)}) \\cdot A^{(L-1)}\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial{W^{(L-1)}}} = -2(Y-\\hat{Y}) \\cdot \\sigma^{'}(Z^{(L)}) \\cdot W^{(L)} \\cdot \\sigma^{'}(Z^{(L-1)}) \\cdot X\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "As a general note, in case we have further hidden layers, this repeated application of chain rule differentiation is continued until we get the entire cost gradient vector. This can be automated easily enough, but for simplicity's sake we keep our implementation limited to the three layers for now. \n",
    "\n",
    "All that remains is to code up the cost function, its gradient function, and the backpropagation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost(y, yhat):\n",
    "    return np.sum(np.square(y - yhat))\n",
    "\n",
    "def cost_prime(y, yhat):\n",
    "    return -1*(y - yhat)\n",
    "\n",
    "def sigmoid_prime(z_vector):\n",
    "    return np.exp(np.multiply(z_vector, -1))/((1+np.exp(np.multiply(z_vector, -1)))**2)\n",
    "\n",
    "def backprop(neuralnet, output_vector):\n",
    "    \n",
    "    # Equation 1\n",
    "    dCdZ_L = np.multiply(cost_prime(output_vector, neuralnet.activation_vectors[-1]), sigmoid_prime(neuralnet.z_vectors[-1]))\n",
    "    dCdW_L = np.dot(np.transpose(neuralnet.activation_vectors[-2][0]), dCdZ_L[0])\n",
    "    \n",
    "    # Equation 2\n",
    "    dCdZ_L1 = np.dot(dCdZ_L[0], np.transpose(neuralnet.weight_list[-1])) * sigmoid_prime(neuralnet.z_vectors[-2])\n",
    "    dCdW_L1 = np.dot(np.transpose(neuralnet.activation_vectors[0][0]), dCdZ_L1[0])\n",
    "\n",
    "    return np.concatenate((dCdW_L1.ravel(), dCdW_L.ravel()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63918724]\n",
      " [ 0.71035219]\n",
      " [ 0.62305282]\n",
      " [ 0.6966617 ]]\n"
     ]
    }
   ],
   "source": [
    "#Test code  \n",
    "x = np.array([ [0,0], [0,1], [1,0], [1,1]])  \n",
    "y = np.array([ [0], [1], [1], [0]])\n",
    "\n",
    "neuralnet = Neuralnetwork([x.shape[1], 3, y.shape[1]])\n",
    "yhat = forwardprop(neuralnet, x)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Gradient Descent\n",
    "\n",
    "The function of the backpropagation algorithm is to propagate the error backwards through the neural network layers, and thereby calculate the gradient of all the weight vectors. Now we know the slope of the cost function with respect to each weight in the network; i.e., for a tiny change in the any particular weight, we can get the corresponding change in cost value using the gradient. Since we are hoping to minimise the cost function, we need to adjust each of the weights in the direction that causes the cost to decrease. \n",
    "\n",
    "This direction is evident from the sign of the gradient for that particular weight. If gradient for a weight is negative, we need to increase that weight so as to bring down cost, and vice-versa. And larger the magnitude of the cost gradient with respect to a weight, more sensitive is the cost to any change in that weight value.\n",
    "\n",
    "So we know the direction in which we need to adjust each weight, but what should be the magnitude of that change? This is a tricky choice, and one that is governed by heuristics of the problem. For now, we will ignore this complication and assume that we decrease each weight by the value of its cost gradient itself. \n",
    "\n",
    "We repeatedly apply forward and backpropagation to minimize the cost iteratively, until we tune the weights to get close to the expected output. This raises the question of how many iterations we need to apply. Here again, we arbitrarily choose the number of iterations of minimzation to 1000 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0  Error:  1.1198828924\n",
      "Iteration  50  Error:  0.998908339967\n",
      "Iteration  100  Error:  0.988071943421\n",
      "Iteration  150  Error:  0.955326680799\n",
      "Iteration  200  Error:  0.886863521292\n",
      "Iteration  250  Error:  0.792638388888\n",
      "Iteration  300  Error:  0.621937555098\n",
      "Iteration  350  Error:  0.393281650914\n",
      "Iteration  400  Error:  0.238346177146\n",
      "Iteration  450  Error:  0.158803057708\n",
      "Iteration  500  Error:  0.116031440962\n",
      "Iteration  550  Error:  0.0904848377332\n",
      "Iteration  600  Error:  0.0738091891523\n",
      "Iteration  650  Error:  0.0621674771405\n",
      "Iteration  700  Error:  0.0536176718672\n",
      "Iteration  750  Error:  0.0470892897217\n",
      "Iteration  800  Error:  0.0419494885636\n",
      "Iteration  850  Error:  0.0378022438051\n",
      "Iteration  900  Error:  0.0343879028864\n",
      "Iteration  950  Error:  0.0315295189939\n"
     ]
    }
   ],
   "source": [
    "def train(neuralnet, X, y, iterations = 1000):\n",
    "    \n",
    "    for loopcounter in range(0, iterations):\n",
    "        yhat = forwardprop(neuralnet, X)\n",
    "        grad = backprop(neuralnet, y)\n",
    "        #print((neuralnet.weight_list[0].ravel(),neuralnet.weight_list[1].ravel()))\n",
    "        #print(grad)\n",
    "        if loopcounter%50 == 0:\n",
    "            print(\"Iteration \", loopcounter, \" Error: \", cost(y, yhat))\n",
    "        W1_start = 0\n",
    "        W1_end = neuralnet.hidden_layer_size * neuralnet.input_layer_size        \n",
    "        W1 = np.reshape(grad[W1_start:W1_end], (neuralnet.input_layer_size , neuralnet.hidden_layer_size))\n",
    "        \n",
    "        W2_end = W1_end + neuralnet.hidden_layer_size*neuralnet.output_layer_size\n",
    "        W2 = np.reshape(grad[W1_end:W2_end], (neuralnet.hidden_layer_size, neuralnet.output_layer_size))\n",
    "        neuralnet.weight_list = [neuralnet.weight_list[0] - W1, neuralnet.weight_list[1] - W2]\n",
    "        #print((neuralnet.weight_list[0].ravel(),neuralnet.weight_list[1].ravel()))\n",
    "    return\n",
    "\n",
    "#Test code  \n",
    "x = np.array([ [0,0], [0,1], [1,0], [1,1]])  \n",
    "y = np.array([ [0], [1], [1], [0]])\n",
    "neuralnet = Neuralnetwork([x.shape[1], 3, y.shape[1]])\n",
    "train(neuralnet, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.11965205],\n",
      "       [ 0.92065739],\n",
      "       [ 0.92086371],\n",
      "       [ 0.04767659]])]\n"
     ]
    }
   ],
   "source": [
    "# Final output\n",
    "print(neuralnet.activation_vectors[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning our neural network design\n",
    "\n",
    "So we observe here that using our simple training algorithm, we managed to progressively reduce the error to get closer to the required output. However, with our naive training algorithm, the convergence is extremely slow and our outputs do not get close to our required outputs fast enough. Can we do better? Quite possibly, if we give more thought to designing better training algorithms.\n",
    "\n",
    "So far, we've made the following assumptions as we progressed through our understanding of how neural networks worked.\n",
    "* The activation function was assumed to be sigmoid because it met our requirement to map an infinite interval input into $[0,1]$.\n",
    "* The weights were initialised in a random fashion.\n",
    "* The number of hidden layers was chosen to be 1 for simplicity's sake.\n",
    "* The number of neurons in the hidden layer was arbitrarily chosen.\n",
    "* The absolute magnitude by which we adjusted the weights was assumed to be the same as its cost gradient value.\n",
    "* The number of iterations for reducing cost was randomly chosen to be 1000.\n",
    "* The algorithm we used to train our network is simplistic, and we may be able to do better.\n",
    "\n",
    "In the next section, we will revisit each of these assumptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
